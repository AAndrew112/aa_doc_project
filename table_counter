#!/usr/bin/env python3
from pathlib import Path
import argparse
import csv
import sys
from datetime import datetime
from docx import Document

def timestamped_outpath(out_path: Path) -> Path:
    ts = datetime.now().strftime("%m%d%Y_%H%M")
    stem, suf = out_path.stem, (out_path.suffix or ".csv")
    return out_path.with_name(f"{stem}__{ts}{suf}")

def _preview_from_table(tbl, max_chars: int = 100) -> str:
    """Build a short text preview from the first 1–2 rows."""
    parts = []
    try:
        take_rows = min(2, len(tbl.rows))
        for r in range(take_rows):
            row_text = " | ".join(c.text.strip() for c in tbl.rows[r].cells)
            if row_text:
                parts.append(row_text)
    except Exception:
        pass
    preview = " / ".join(parts).strip()
    if len(preview) > max_chars:
        preview = preview[:max_chars - 1] + "…"
    return preview

def _count_rows_cols(tbl):
    """Safe row/column counts."""
    try:
        rows = len(tbl.rows)
    except Exception:
        rows = ""
    try:
        cols = len(tbl.columns)
    except Exception:
        # Fallback: infer from first row
        try:
            cols = len(tbl.rows[0].cells)
        except Exception:
            cols = ""
    return rows, cols

def _walk_tables(tables, location_label: str, results: list, file_path: Path, base_index: int, depth: int = 0):
    """Record tables and (optionally) nested tables recursively."""
    idx = base_index
    for tbl in tables:
        rows, cols = _count_rows_cols(tbl)
        preview = _preview_from_table(tbl)

        results.append({
            "File Path": str(file_path),
            "Table # (per file)": idx,
            "Location": location_label,
            "Nested Depth": depth,
            "Rows": rows,
            "Columns": cols,
            "Preview": preview
        })
        idx += 1

        # Walk nested tables inside cells
        for row in tbl.rows:
            for cell in row.cells:
                if getattr(cell, "tables", None):
                    idx = _walk_tables(cell.tables, location_label, results, file_path, idx, depth + 1)
    return idx

def locate_tables_in_docx(path: Path, include_hf: bool):
    """
    Return a list of table records for a single docx:
    - Location: Body / Header (Section N) / Footer (Section N)
    - Nested Depth: 0 = top-level, >0 = nested inside cells
    - Rows, Columns, Preview
    """
    doc = Document(path)
    results = []
    next_idx = 1

    # Body tables
    next_idx = _walk_tables(doc.tables, "Body", results, path, next_idx, depth=0)

    # Headers/Footers per section
    if include_hf:
        for s_idx, section in enumerate(doc.sections, start=1):
            if section.header:
                next_idx = _walk_tables(section.header.tables, f"Header (Section {s_idx})", results, path, next_idx, depth=0)
            if section.footer:
                next_idx = _walk_tables(section.footer.tables, f"Footer (Section {s_idx})", results, path, next_idx, depth=0)

    return results

def main():
    p = argparse.ArgumentParser(
        description="Locate every table in .docx files with location, nested depth, row/col counts, and a text preview."
    )
    p.add_argument("--root", type=Path, required=True, help="Root folder to search (recursively).")
    p.add_argument("--out", type=Path, default=Path("docx_table_locator.csv"),
                   help="Base output CSV filename (timestamp appended).")
    p.add_argument("--no-headers-footers", action="store_true",
                   help="Exclude tables in headers/footers.")
    p.add_argument("--no-timestamp", action="store_true",
                   help="Do NOT append timestamp to output filename.")
    args = p.parse_args()

    if not args.root.exists():
        p.error(f"--root path does not exist: {args.root}")

    include_hf = not args.no_headers_footers
    out_path = args.out if args.no_timestamp else timestamped_outpath(args.out)

    # [1/5] start + indexing
    files = list(args.root.rglob("*.docx"))
    print(f"[1/5] Indexed {len(files)} .docx files under {args.root}")

    headers = ["File Path", "Table # (per file)", "Location", "Nested Depth", "Rows", "Columns", "Preview"]
    all_rows = []
    total = len(files)

    if total == 0:
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with out_path.open("w", newline="", encoding="utf-8") as fp:
            writer = csv.DictWriter(fp, fieldnames=headers)
            writer.writeheader()
        print(f"[5/5] Done. Output: {out_path}")
        return

    # Progress checkpoints: 25%, 50%, 75%
    checkpoints = {0.25, 0.50, 0.75}
    fired = set()

    for idx, f in enumerate(files, start=1):
        try:
            all_rows.extend(locate_tables_in_docx(f, include_hf=include_hf))
        except Exception as e:
            print(f"[error] {f}: {e}", file=sys.stderr)

        # [2-4/5] milestones
        progress = idx / total
        for cp in (0.25, 0.50, 0.75):
            if progress >= cp and cp not in fired:
                pct = int(cp * 100)
                label = 2 if cp == 0.25 else 3 if cp == 0.50 else 4
                print(f"[{label}/5] ~{pct}% complete ({idx}/{total})")
                fired.add(cp)

    # Write CSV
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", newline="", encoding="utf-8") as fp:
        writer = csv.DictWriter(fp, fieldnames=headers)
        writer.writeheader()
        writer.writerows(all_rows)

    print(f"[5/5] Done. Wrote {len(all_rows)} rows → {out_path}")

if __name__ == "__main__":
    main()
