# analyze.py
"""
Purpose
-------
Analyze preprocessed documents in two stages:
  1) Dedupe: compute TF-IDF + cosine similarity, keep pairs ≥ threshold,
     and group docs into clusters (connected components).
  2) Complexity (optional): compute a rough 1–5 score per doc.

Key behavior
------------
• Filenames include both the threshold and a timestamp:
    e.g., clusters_t80__YYYYMMDD_HHMM.csv
• A single combined, human-readable report is written per threshold:
    report_t80__YYYYMMDD_HHMM.txt
• No run_manifest.json is produced (kept simple and human-oriented).

Usage (if running this file directly)
-------------------------------------
py analyze.py --thresh 80 90
(Or call run_dedupe / run_complexity from orchestrate.py, which is recommended.)
"""

from __future__ import annotations
from pathlib import Path
import argparse
import re
import datetime
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer   # text → numeric features
from sklearn.metrics.pairwise import cosine_similarity         # cosine between vectors

DATA_DIR = Path("data")  # default output root


# ---------- Helper: union-find to turn pairs into clusters ----------
def clusters_from_pairs(pairs: list[tuple[str, str]], ids: list[str]) -> list[list[str]]:
    """
    Convert pairwise links (A~B) into connected components (clusters).
    If A~B and B~C, A,B,C land in the same cluster (even if A~C wasn't directly scored).
    """
    parent = {i: i for i in ids}  # start with each doc as its own parent

    def find(x: str) -> str:
        # Follow parent pointers to the root; compress path for speed
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(a: str, b: str) -> None:
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    for a, b in pairs:
        union(a, b)

    groups: dict[str, list[str]] = {}
    for i in ids:
        root = find(i)
        groups.setdefault(root, []).append(i)
    return list(groups.values())


# ---------- Stage 1: DEDUPE ----------
def run_dedupe(
    chunks_csv: Path,
    out_dir: Path,
    thresh_float: float,
    min_df: int = 2,
    ngram_range: tuple[int, int] = (1, 2),
    batch: int = 1500,
) -> dict[str, Path]:
    """
    Find similar documents and build clusters.

    Returns
    -------
    dict with:
      'clusters_csv' : Path to clusters CSV (timestamped)
      'report_path'  : Path to the combined report (started here)
      'pairs_csv'    : Path to similar pairs CSV (timestamped)
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
    t_int = int(thresh_float * 100)                     # e.g., 0.80 → 80
    suffix = f"t{t_int}__{timestamp}"                   # used in all filenames

    # 1) Rebuild full documents from chunks (group by doc_id and join text)
    df = pd.read_csv(chunks_csv)
    if not {"doc_id", "text_chunk"}.issubset(df.columns):
        raise ValueError("chunks CSV must include 'doc_id' and 'text_chunk'")
    docs = (
        df.groupby("doc_id", sort=False)["text_chunk"]
          .apply(lambda s: " ".join(map(str, s.fillna(""))))  # join chunks → one string per doc
          .reset_index()
    )
    ids: list[str] = docs["doc_id"].tolist()
    texts: list[str] = docs["text_chunk"].fillna("").tolist()
    n = len(ids)

    # 2) Vectorize text with TF-IDF (unigrams + bigrams, english stopwords)
    vect = TfidfVectorizer(stop_words="english", ngram_range=ngram_range, min_df=min_df)
    X = vect.fit_transform(texts)  # sparse matrix, memory-efficient

    # 3) Compute cosine similarities in batches (avoid dense n×n matrix)
    pairs: list[tuple[str, str, float]] = []
    for start in range(0, n, batch):
        end = min(start + batch, n)
        sims = cosine_similarity(X[start:end], X)  # block vs all docs
        for i in range(end - start):
            gi = start + i
            row = sims[i]
            for j in range(gi + 1, n):            # upper triangle only (skip self + dupes)
                s = row[j]
                if s >= thresh_float:
                    pairs.append((ids[gi], ids[j], float(s)))

    # 4) Save all qualifying pairs (A,B,similarity) for auditability
    pairs_df = pd.DataFrame(pairs, columns=["doc_id_a", "doc_id_b", "similarity"]) \
                 .sort_values("similarity", ascending=False)
    pairs_path = out_dir / f"similar_pairs_{suffix}.csv"
    pairs_df.to_csv(pairs_path, index=False)

    # 5) Build clusters (connected components) from pair links
    comps = clusters_from_pairs([(a, b) for a, b, _ in pairs], ids) if pairs else [[i] for i in ids]
    rows = []
    for cid, members in enumerate(sorted(comps, key=len, reverse=True), start=1):
        for m in members:
            rows.append({"cluster_id": cid, "doc_id": m, "cluster_size": len(members)})
    clust_df = pd.DataFrame(rows)

    # 6) Add average intra-cluster similarity (mean of pair scores within cluster)
    if not pairs_df.empty:
        doc_to_cluster = clust_df.set_index("doc_id")["cluster_id"].to_dict()
        pairs_df["cluster_id"] = pairs_df["doc_id_a"].map(doc_to_cluster)  # map by A's cluster
        # average similarity per cluster id
        rel = (
            pairs_df.groupby("cluster_id")["similarity"]
                    .mean().reset_index().rename(columns={"similarity": "avg_similarity"})
        )
        clust_df = clust_df.merge(rel, on="cluster_id", how="left")
    clust_df["avg_similarity"] = clust_df["avg_similarity"].fillna(0.0)

    clusters_path = out_dir / f"clusters_{suffix}.csv"
    clust_df.to_csv(clusters_path, index=False)

    # 7) Human-readable summary report (no JSON manifest)
    redundant_docs = (
        set(pairs_df["doc_id_a"]).union(set(pairs_df["doc_id_b"]))
        if not pairs_df.empty else set()
    )
    summary = {
        "docs_compared": n,
        "pairs_ge_thresh": int(len(pairs_df)),
        "redundant_docs": int(len(redundant_docs)),
        "redundancy_pct": (len(redundant_docs) / max(1, n)) * 100.0,
        "clusters": int(clust_df["cluster_id"].nunique()),
        "largest_cluster": int(clust_df["cluster_size"].max()),
        "mean_cluster_relatedness": float(
            clust_df.drop_duplicates("cluster_id")["avg_similarity"].mean()
        ),
    }

    report_path = out_dir / f"report_{suffix}.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("[SIMILARITY SUMMARY]\n")
        for k, v in summary.items():
            f.write(f"{k}: {v}\n")

    print(f"[dedupe {suffix}] pairs={len(pairs_df)} clusters={summary['clusters']}")
    return {"clusters_csv": clusters_path, "report_path": report_path, "pairs_csv": pairs_path}


# ---------- Stage 2: COMPLEXITY (optional) ----------
# Note: You said you might skip complexity. This function remains available
# for ad-hoc runs or future needs, but your orchestrator can simply not call it.

def count_conditionals(text: str) -> int:
    return len(re.findall(r"\b(if|unless|provided|except|whereas)\b", text, flags=re.I))

def count_statutes(text: str) -> int:
    return len(re.findall(r"\b(ORS|U\.S\.C\.|§)\b", text))

def avg_sentence_length(text: str) -> float:
    """Simple proxy for readability: average words per sentence."""
    sents = re.split(r"[.!?]", text)
    words = [len(re.findall(r"\w+", s)) for s in sents if s.strip()]
    return float(np.mean(words)) if words else 0.0

def run_complexity(clean_csv: Path, clusters_csv: Path, out_dir: Path, report_path: Path | None = None) -> Path:
    """
    Compute a 1–5 complexity score per doc, join with clusters, and
    append a short section to the existing report. Returns the report path.
    """
    out_dir.mkdir(parents=True, exist_ok=True)

    # Derive suffix from clusters filename to keep names consistent
    stem = Path(clusters_csv).stem                 # e.g., "clusters_t80__20251008_1345"
    suffix = stem.replace("clusters_", "")        # → "t80__20251008_1345"

    # Load cleaned texts and cluster assignments
    df = pd.read_csv(clean_csv)
    cl = pd.read_csv(clusters_csv)
    df = df.merge(cl[["doc_id", "cluster_id", "cluster_size"]], on="doc_id", how="left")
    df["cluster_size"].fillna(1, inplace=True)

    # Count basic features that track with complexity
    df["cond_count"] = df["text_clean"].fillna("").apply(count_conditionals)
    df["stat_count"] = df["text_clean"].fillna("").apply(count_statutes)
    df["sent_len"]   = df["text_clean"].fillna("").apply(avg_sentence_length)

    # Corpus averages for comparison
    df["cond_avg"] = df["cond_count"].mean()
    df["stat_avg"] = df["stat_count"].mean()
    df["sent_avg"] = df["sent_len"].mean()

    # Score: base on length, +1 for above-average legalese / sentence length
    def complexity_score(row) -> int:
        wc = row.get("word_count_clean", 0)
        base = min(5, max(1, int(np.ceil(wc / 700))))  # ~1 point per 700 words
        score = base
        if row["cond_count"] > row["cond_avg"]: score += 1
        if row["stat_count"] > row["stat_avg"]: score += 1
        if row["sent_len"]  > row["sent_avg"]:  score += 1
        return int(min(score, 5))

    df["complexity"] = df.apply(complexity_score, axis=1)
    df.to_csv(out_dir / f"doc_complexity_{suffix}.csv", index=False)

    # Append a concise section to the same threshold report
    if report_path and Path(report_path).exists():
        rp = Path(report_path)
        header = "\n[COMPLEXITY SUMMARY]\n"
    else:
        rp = out_dir / f"report_{suffix}.txt"
        header = "[COMPLEXITY SUMMARY]\n"

    exec_summary = (
        f"Total documents: {len(df)}\n"
        f"Unique clusters: {df['cluster_id'].nunique()}\n"
        f"Docs with duplicates: {(df['cluster_size']>1).sum()} "
        f"({((df['cluster_size']>1).mean()*100):.1f}%)\n"
        f"Average complexity: {df['complexity'].mean():.2f}\n"
        f"Cluster size distribution (top):\n{df['cluster_size'].value_counts().head(10)}\n"
    )

    with open(rp, "a" if rp.exists()
