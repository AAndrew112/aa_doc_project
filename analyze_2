"""
analyze.py
Runs the *analysis* stages:
  - dedupe:    TF-IDF + cosine similarity → pairs ≥ threshold → clusters (with avg_similarity)
  - complexity: simple 1–5 score per document

Outputs are written under subfolders like:
  data/t80/, data/t90/
So you can compare thresholds side by side.

USAGE EXAMPLES
--------------
# Run both stages for thresholds 80 and 90:
python src/analyze.py --thresh 80 90

# Only dedupe at t=90:
python src/analyze.py --run dedupe --thresh 90
"""

from __future__ import annotations
from pathlib import Path
import argparse, json, re
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer  # pip install scikit-learn
from sklearn.metrics.pairwise import cosine_similarity

DATA_DIR = Path("data")

# ---------- clustering helper (union-find) ----------

def clusters_from_pairs(pairs, ids):
    """
    Turn edges (docA, docB) into connected components (clusters).
    If A~B and B~C, then A,B,C land in the same cluster (even if A~C wasn’t directly scored).
    """
    parent = {i: i for i in ids}

    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]  # path compression for speed
            x = parent[x]
        return x

    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    for a, b in pairs:
        union(a, b)

    groups = {}
    for i in ids:
        root = find(i)
        groups.setdefault(root, []).append(i)
    return list(groups.values())

# ---------- DEDUPE (similarity + clusters) ----------

def run_dedupe(chunks_csv: Path, out_dir: Path, thresh_float: float,
               min_df: int = 2, ngram_range=(1, 2), batch: int = 1500):
    """
    - Read chunked text, rebuild *doc-level* strings
    - Vectorize with TF-IDF (unigrams + bigrams, english stopwords, min_df=2)
    - Compute cosine similarities in manageable batches
    - Keep only pairs ≥ threshold
    - Build clusters (connected components)
    - Compute 'avg_similarity' per cluster = mean of pairwise scores in that cluster
    """
    out_dir.mkdir(parents=True, exist_ok=True)

    # 1) Read and rebuild documents from chunks
    df = pd.read_csv(chunks_csv)
    if not {"doc_id", "text_chunk"}.issubset(df.columns):
        raise ValueError("chunks CSV must include 'doc_id' and 'text_chunk'")

    docs = (
        df.groupby("doc_id", sort=False)["text_chunk"]
          .apply(lambda s: " ".join(map(str, s.fillna(""))))
          .reset_index()
    )
    ids   = docs["doc_id"].tolist()
    texts = docs["text_chunk"].fillna("").tolist()
    n = len(ids)

    # 2) TF-IDF vectors (sparse matrix, memory efficient)
    vect = TfidfVectorizer(stop_words="english", ngram_range=ngram_range, min_df=min_df)
    X = vect.fit_transform(texts)

    # 3) Batched cosine similarity (avoid building a huge n x n dense matrix)
    pairs = []  # list of (doc_id_a, doc_id_b, similarity)
    for start in range(0, n, batch):
        end = min(start + batch, n)
        sims = cosine_similarity(X[start:end], X)  # block vs all
        for i in range(end - start):
            gi = start + i            # global index
            row = sims[i]
            for j in range(gi + 1, n):  # upper triangle only (skip self + dupes)
                s = row[j]
                if s >= thresh_float:
                    pairs.append((ids[gi], ids[j], float(s)))

    # 4) Save the similar pairs table (for auditing)
    pairs_df = pd.DataFrame(pairs, columns=["doc_id_a", "doc_id_b", "similarity"]) \
                 .sort_values("similarity", ascending=False)
    (out_dir / "similar_pairs.csv").parent.mkdir(parents=True, exist_ok=True)
    pairs_df.to_csv(out_dir / "similar_pairs.csv", index=False)

    # 5) Build clusters (connected components)
    comps = clusters_from_pairs([(a, b) for a, b, _ in pairs], ids) if pairs else [[i] for i in ids]
    rows = []
    for cid, members in enumerate(sorted(comps, key=len, reverse=True), start=1):
        for m in members:
            rows.append({"cluster_id": cid, "doc_id": m, "cluster_size": len(members)})
    clust_df = pd.DataFrame(rows)

    # 6) Compute avg_similarity per cluster
    if not pairs_df.empty:
        # Map docs to their cluster IDs, then average similarity for pairs within each cluster
        doc_to_cluster = clust_df.set_index("doc_id")["cluster_id"].to_dict()
        pairs_df["cluster_id"] = pairs_df["doc_id_a"].map(doc_to_cluster)
        rel = (pairs_df.groupby("cluster_id")["similarity"]
                      .mean().reset_index().rename(columns={"similarity": "avg_similarity"}))
        clust_df = clust_df.merge(rel, on="cluster_id", how="left")
    clust_df["avg_similarity"] = clust_df["avg_similarity"].fillna(0.0)

    clust_df.to_csv(out_dir / "clusters.csv", index=False)

    # 7) Small text summary and a machine-readable manifest
    redundant_docs = set(pairs_df["doc_id_a"]).union(set(pairs_df["doc_id_b"])) if not pairs_df.empty else set()
    summary = {
        "docs_compared": n,
        "pairs_ge_thresh": int(len(pairs_df)),
        "redundant_docs": int(len(redundant_docs)),
        "redundancy_pct": (len(redundant_docs) / max(1, n)) * 100.0,
        "clusters": int(clust_df["cluster_id"].nunique()),
        "largest_cluster": int(clust_df["cluster_size"].max()),
        "mean_cluster_relatedness": float(clust_df.drop_duplicates("cluster_id")["avg_similarity"].mean())
    }
    (out_dir / "similarity_summary.txt").write_text(
        "\n".join(f"{k}: {v}" for k, v in summary.items()), encoding="utf-8"
    )
    (out_dir / "run_manifest.json").write_text(
        json.dumps({
            "stage": "dedupe",
            "thresh": thresh_float,
            "min_df": min_df,
            "ngram_range": ngram_range,
            "batch": batch,
            "inputs": {"chunks_csv": str(chunks_csv)},
            "summary": summary
        }, indent=2),
        encoding="utf-8"
    )
    print(f"[dedupe t{int(thresh_float*100)}] pairs={len(pairs_df)} clusters={summary['clusters']}")

# ---------- COMPLEXITY (simple 1–5 heuristic) ----------

def count_conditionals(text): return len(re.findall(r"\b(if|unless|provided|except|whereas)\b", text, flags=re.I))
def count_statutes(text):     return len(re.findall(r"\b(ORS|U\.S\.C\.|§)\b", text))
def avg_sentence_length(text):
    sents = re.split(r"[.!?]", text)
    words = [len(re.findall(r"\w+", s)) for s in sents if s.strip()]
    return float(np.mean(words)) if words else 0.0

def run_complexity(clean_csv: Path, clusters_csv: Path, out_dir: Path):
    """
    Join the cleaned documents with their clusters, then compute:
      - counts of conditional words and statute references
      - average sentence length (rough readability)
      - a 1–5 complexity score (longer + more legalese → higher)
    """
    out_dir.mkdir(parents=True, exist_ok=True)

    df = pd.read_csv(clean_csv)
    cl = pd.read_csv(clusters_csv)

    # Bring cluster info onto each document row
    df = df.merge(cl[["doc_id", "cluster_id", "cluster_size"]], on="doc_id", how="left")
    df["cluster_size"].fillna(1, inplace=True)

    # Feature counts for the heuristic
    df["cond_count"] = df["text_clean"].fillna("").apply(count_conditionals)
    df["stat_count"] = df["text_clean"].fillna("").apply(count_statutes)
    df["sent_len"]   = df["text_clean"].fillna("").apply(avg_sentence_length)

    # Corpus averages (used as soft thresholds)
    df["cond_avg"] = df["cond_count"].mean()
    df["stat_avg"] = df["stat_count"].mean()
    df["sent_avg"] = df["sent_len"].mean()

    # Convert word count → base score, then add points if above-average legalese/length
    def complexity_score(row):
        wc = row["word_count_clean"]
        base = min(5, max(1, int(np.ceil(wc / 700))))  # 1 per ~700 words, capped 1–5
        score = base
        if row["cond_count"] > row["cond_avg"]: score += 1
        if row["stat_count"] > row["stat_avg"]: score += 1
        if row["sent_len"]  > row["sent_avg"]:  score += 1
        return int(min(score, 5))

    df["complexity"] = df.apply(complexity_score, axis=1)

    df.to_csv(out_dir / "doc_complexity.csv", index=False)

    exec_summary = (
        f"Total documents: {len(df)}\n"
        f"Unique clusters: {df['cluster_id'].nunique()}\n"
        f"Docs with duplicates: {(df['cluster_size']>1).sum()} "
        f"({((df['cluster_size']>1).mean()*100):.1f}%)\n"
        f"Average complexity: {df['complexity'].mean():.2f}\n"
        f"Cluster size distribution (top):\n{df['cluster_size'].value_counts().head(10)}\n"
    )
    (out_dir / "executive_summary.txt").write_text(exec_summary, encoding="utf-8")
    print(f"[complexity] wrote doc_complexity.csv and executive_summary.txt in {out_dir}")

# ---------- command-line entry point ----------

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--run", type=str, default="dedupe,complexity",
                        help="Which stages to run (comma list)")
    parser.add_argument("--thresh", type=int, nargs="+", default=[80, 90],
                        help="One or more thresholds as integers, e.g., 80 90")
    parser.add_argument("--chunks", type=str, default="data/corpus_chunks.csv")
    parser.add_argument("--clean", type=str, default="data/corpus_clean.csv")
    parser.add_argument("--min-df", type=int, default=2)
    parser.add_argument("--ngrams", type=str, default="1,2")
    parser.add_argument("--batch", type=int, default=1500)
    args = parser.parse_args()

    ngrams = tuple(int(x) for x in args.ngrams.split(","))
    stages = [s.strip().lower() for s in args.run.split(",") if s.strip()]
    chunks_csv = Path(args.chunks)
    clean_csv  = Path(args.clean)

    for t_int in args.thresh:
        t = float(t_int) / 100.0             # turn 80 -> 0.80
        out_dir = DATA_DIR / f"t{t_int}"     # write results under data/t80/, data/t90/, etc.

        if "dedupe" in stages:
            run_dedupe(chunks_csv, out_dir, t, min_df=args.min_df, ngram_range=ngrams, batch=args.batch)

        if "complexity" in stages:
            # depends on clusters created above for this threshold
            run_complexity(clean_csv, out_dir / "clusters.csv", out_dir)

if __name__ == "__main__":
    main()
